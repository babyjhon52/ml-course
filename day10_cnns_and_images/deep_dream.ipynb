{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAv_IoHU6poF"
   },
   "source": [
    "### Deep dream in PyTorch\n",
    "\n",
    "However, you can also use any model from VGG family from torchvision.models.\n",
    "\n",
    "Please note, that any other model from torchvision.models might not work properly with this code.\n",
    "\n",
    "That's what happens when you play with some ideas, that are proposed [here](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6K3pbm1V6poO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.ndimage as nd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hbx6vD1u6poQ"
   },
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    img = tensor[0].detach().cpu()\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return img\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "!wget -q https://i.kym-cdn.com/entries/icons/mobile/000/030/157/womanyellingcat.jpg -O data/my_img.jpg\n",
    "!wget -q https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/450px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg -O data/art.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YTxJzBo6poW"
   },
   "source": [
    "# Let's Dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Normalize(mean, std)\n",
    "\n",
    "\n",
    "def denormalize(image_np):\n",
    "    image_np = image_np[0]\n",
    "    image_np = image_np.transpose(1, 2, 0)\n",
    "    return np.clip(image_np, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def dream_with_model(image: np.ndarray, model: nn.Module, iterations: int, lr: float=0.01):\n",
    "    \"\"\" Updates the image to maximize outputs for n iterations \"\"\"\n",
    "    # set up trainable input tensor\n",
    "    in_tensor = torch.tensor(image, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    \n",
    "    # apply data normalization to match preprocessing from pretrained model\n",
    "    in_tensor.data = preprocess(in_tensor.data)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        model.zero_grad()\n",
    "        out = model(in_tensor)\n",
    "        \n",
    "        loss = out.abs().sum() # .norm()\n",
    "        loss.backward()\n",
    "        \n",
    "        avg_grad = in_tensor.grad.abs().cpu().mean().item()\n",
    "        norm_lr = lr / avg_grad\n",
    "        \n",
    "        in_tensor.data += norm_lr * in_tensor.grad\n",
    "        in_tensor.data.clamp_(-2.0, 2.0)\n",
    "        in_tensor.grad.zero_()\n",
    "    output = in_tensor.detach().cpu().numpy()\n",
    "    output = output * std[None, :, None, None] + mean[None, :, None, None]\n",
    "    return output\n",
    "\n",
    "\n",
    "def deep_dream(image, model, iterations, lr, octave_scale, num_octaves):\n",
    "    \"\"\" Main Deep Dream processing \"\"\"\n",
    "    np_image_bchw = image.transpose(2, 0, 1)[None] / 255.\n",
    "\n",
    "    # Extract image representations for each octave\n",
    "    octaves = [np_image_bchw]\n",
    "    for idx in range(num_octaves - 1):\n",
    "        zoom = (1, 1, 1 / octave_scale, 1 / octave_scale)\n",
    "        zoomed_octave = nd.zoom(octaves[-1], zoom=zoom, order=1)\n",
    "        octaves.append(zoomed_octave)\n",
    "\n",
    "    detail = np.zeros_like(octaves[-1])\n",
    "    for octave, octave_base in enumerate(octaves[::-1]):\n",
    "        if octave > 0:\n",
    "            # Upsample detail to new octave dimension\n",
    "            octave_inv_zoom = np.array(octave_base.shape) / np.array(detail.shape)\n",
    "            detail = nd.zoom(detail, zoom=octave_inv_zoom, order=1)\n",
    "        \n",
    "        # Add deep dream detail from previous octave to new base\n",
    "        input_image = octave_base + detail\n",
    "        # Get new deep dream image\n",
    "        dreamed_image = dream_with_model(input_image, model, iterations, lr)\n",
    "        # Extract deep dream details\n",
    "        detail = dreamed_image - octave_base\n",
    "    \n",
    "    return denormalize(dreamed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1Vg4K12BUWd"
   },
   "outputs": [],
   "source": [
    "# you can play with the parameters\n",
    "layer_num = 32\n",
    "iterations = 100\n",
    "octave_scale = 1.3\n",
    "num_octaves = 2\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j62lbGEtBgA3"
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "dst_size = (512, 256) # (w, h)\n",
    "image = Image.open(\"data/my_img.jpg\").resize(dst_size)\n",
    "# image = Image.open(\"data/art.jpg\").resize(dst_size)\n",
    "image = np.array(image)\n",
    "\n",
    "# Define the model\n",
    "network = models.vgg19(pretrained=True)\n",
    "layers = list(network.features.children())\n",
    "\n",
    "model = nn.Sequential(*layers[: (layer_num + 1)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-r9XVk-BjoA"
   },
   "outputs": [],
   "source": [
    "dreamed_image = deep_dream(\n",
    "    image,\n",
    "    model,\n",
    "    iterations=iterations,\n",
    "    lr=lr,\n",
    "    octave_scale=octave_scale,\n",
    "    num_octaves=num_octaves\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UCva8SPBjrU"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(dreamed_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "style_transfer_pytorch-draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
