{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "79cec962-f93d-486f-b67b-73168472348b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78c15bd9c980>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8_pYr7PZi2Sn"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], 32),\n",
        "    nn.GELU(),\n",
        "    nn.Linear(32, 16),\n",
        "    nn.GELU(),\n",
        "    nn.Linear(16, n_actions)\n",
        ")\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-e_RefALorG",
        "outputId": "dc3d61da-b811-4b73-c754-5bf2c000447f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_probs(states):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array [batch, state_dim] или torch.Tensor\n",
        "    :returns: numpy array [batch, n_actions]\n",
        "    \"\"\"\n",
        "    if isinstance(states, np.ndarray) and states.ndim == 1:\n",
        "        states = states[None, :]\n",
        "    if torch.is_tensor(states) and states.ndim == 1:\n",
        "        states = states.unsqueeze(0)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = states if torch.is_tensor(states) else torch.as_tensor(states)\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs.detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "5sdENWJAi2Sz"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "\n",
        "    T = len(rewards)\n",
        "    G = 0.0\n",
        "    cumulative_rewards = np.zeros(T, dtype=np.float32)\n",
        "    for t in range(T - 1, -1, -1):\n",
        "        G = rewards[t] + gamma * G\n",
        "        cumulative_rewards[t] = G\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DX39wcUi2S3",
        "outputId": "77dc4e68-eea3-4c4b-b720-2add75de963d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbwDRp44LorH"
      },
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Oq6r-t5YLorH"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = log_probs.gather(1, actions.long().view(-1, 1)).squeeze(1)  # [B]\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = (log_probs_for_actions * cumulative_returns).mean()  # a number\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = entropy_coef * (-(probs * log_probs).sum(dim=-1).mean())\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    loss = -J_hat - entropy\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckHj5sXBi2TE",
        "outputId": "c186ce9d-a078-44cf-e767-33d09db49cb2",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:25.310\n",
            "mean reward:20.760\n",
            "mean reward:27.690\n",
            "mean reward:39.860\n",
            "mean reward:48.760\n",
            "mean reward:97.710\n",
            "mean reward:247.790\n",
            "mean reward:240.570\n",
            "mean reward:103.690\n",
            "mean reward:141.560\n",
            "mean reward:235.290\n",
            "mean reward:450.130\n",
            "mean reward:241.140\n",
            "mean reward:271.560\n",
            "mean reward:113.860\n",
            "mean reward:155.160\n",
            "mean reward:429.830\n",
            "mean reward:107.130\n",
            "mean reward:183.490\n",
            "mean reward:695.840\n",
            "mean reward:847.310\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "tFWK1k90LorI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "n_actions = env_for_video.action_space.n\n",
        "\n",
        "episode_index = 0\n",
        "step_starting_index = 0\n",
        "\n",
        "obs, info = env_for_video.reset()\n",
        "\n",
        "for step_index in range(800):\n",
        "    probs = predict_probs(np.array([obs]))[0]\n",
        "    action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done or step_index == 799:\n",
        "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "        frames = env_for_video.render()\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        save_video(\n",
        "            frames, \"videos\",\n",
        "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "            step_starting_index=step_starting_index,\n",
        "            episode_index=episode_index,\n",
        "        )\n",
        "        episode_index += 1\n",
        "        step_starting_index = step_index + 1\n",
        "        obs, info = env_for_video.reset()\n",
        "\n",
        "env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p2KVphMLorI"
      },
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T135ih05LorI"
      },
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "2J96g2lnLorI",
        "outputId": "ab958f20-7e4a-4e41-af3b-9d7be58a218f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78c15be06030>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGiCAYAAABd6zmYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAI8RJREFUeJzt3X90lPWB7/HPTH4MJGEmBEzGLMnKXl0xF8EaFGa9Z3evZIk2a7XiXdfL2tRy7REDV6THs2ar9La7e8PBc9pqi7in7YJbq+ylZ9FKpcoGDXUJP4xQA2i0W2qywiQKzUwSksmP+d4/LLOMRjshk3m+M/N+nTPnmOfH5DuPM/PmeeaZJy5jjBEAABZyOz0AAAA+CZECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFjLsUht2rRJl1xyiaZNm6bFixfr4MGDTg0FAGApRyL1z//8z1q3bp2+9rWv6fXXX9fChQtVW1urnp4eJ4YDALCUy4kLzC5evFjXXHONvvvd70qSotGoKioqtGbNGj344IOpHg4AwFK5qf6Fw8PDamtrU2NjY2ya2+1WTU2NWltbx10nEokoEonEfo5Gozpz5oxmzZoll8s15WMGACSXMUZ9fX0qLy+X2/3JB/VSHqkPPvhAY2NjKisri5teVlamt956a9x1mpqa9PWvfz0VwwMApFBXV5fmzJnzifNTHqkL0djYqHXr1sV+DoVCqqysVFdXl7xer4MjAwBciHA4rIqKCs2YMeNTl0t5pGbPnq2cnBx1d3fHTe/u7pbf7x93HY/HI4/H87HpXq+XSAFAGvtdH9mk/Oy+/Px8VVdXq7m5OTYtGo2qublZgUAg1cMBAFjMkcN969atU319vRYtWqRrr71W3/72tzUwMKC77rrLieEAACzlSKRuv/12vf/++1q/fr2CwaCuuuoq/exnP/vYyRQAgOzmyPekJiscDsvn8ykUCvGZFACkoUTfx7l2HwDAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrTThSe/fu1U033aTy8nK5XC49++yzcfONMVq/fr0uvvhiTZ8+XTU1NXrnnXfiljlz5oxWrFghr9er4uJirVy5Uv39/ZN6IACAzDPhSA0MDGjhwoXatGnTuPM3btyoxx57TE888YQOHDigwsJC1dbWamhoKLbMihUrdOzYMe3evVs7d+7U3r179eUvf/nCHwUAIDOZSZBkduzYEfs5Go0av99vHnnkkdi03t5e4/F4zDPPPGOMMeb48eNGkjl06FBsmV27dhmXy2Xee++9hH5vKBQykkwoFJrM8AEADkn0fTypn0mdOHFCwWBQNTU1sWk+n0+LFy9Wa2urJKm1tVXFxcVatGhRbJmamhq53W4dOHBg3PuNRCIKh8NxNwBA5ktqpILBoCSprKwsbnpZWVlsXjAYVGlpadz83NxclZSUxJb5qKamJvl8vtitoqIimcMGAFgqLc7ua2xsVCgUit26urqcHhIAIAWSGim/3y9J6u7ujpve3d0dm+f3+9XT0xM3f3R0VGfOnIkt81Eej0derzfuBgDIfEmN1Ny5c+X3+9Xc3BybFg6HdeDAAQUCAUlSIBBQb2+v2traYsvs2bNH0WhUixcvTuZwAABpLneiK/T39+uXv/xl7OcTJ07oyJEjKikpUWVlpdauXau/+7u/02WXXaa5c+fq4YcfVnl5uW655RZJ0hVXXKEbbrhBd999t5544gmNjIxo9erV+su//EuVl5cn7YEBADLARE8bfPnll42kj93q6+uNMR+ehv7www+bsrIy4/F4zNKlS01HR0fcfZw+fdrccccdpqioyHi9XnPXXXeZvr6+pJ+6CACwU6Lv4y5jjHGwkRckHA7L5/MpFArx+RQApKFE38fT4uw+AEB2IlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGvlOj2AyXjmmWc0ffp0p4cBAJigwcHBhJZL60gZY2SMcXoYAIAJSvS922XS8F0+HA7L5/MpFArJ6/U6PRwAwAQl+j7OZ1IAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtSYUqaamJl1zzTWaMWOGSktLdcstt6ijoyNumaGhITU0NGjWrFkqKirS8uXL1d3dHbdMZ2en6urqVFBQoNLSUj3wwAMaHR2d/KMBAGSUCUWqpaVFDQ0N2r9/v3bv3q2RkREtW7ZMAwMDsWXuv/9+Pf/889q+fbtaWlp08uRJ3XrrrbH5Y2Njqqur0/DwsPbt26cnn3xSW7du1fr165P3qAAAmcFMQk9Pj5FkWlpajDHG9Pb2mry8PLN9+/bYMm+++aaRZFpbW40xxrzwwgvG7XabYDAYW2bz5s3G6/WaSCSS0O8NhUJGkgmFQpMZPgDAIYm+j0/qM6lQKCRJKikpkSS1tbVpZGRENTU1sWXmzZunyspKtba2SpJaW1t15ZVXqqysLLZMbW2twuGwjh07Nu7viUQiCofDcTcAQOa74EhFo1GtXbtW1113nebPny9JCgaDys/PV3FxcdyyZWVlCgaDsWXOD9S5+efmjaepqUk+ny92q6iouNBhAwDSyAVHqqGhQUePHtW2bduSOZ5xNTY2KhQKxW5dXV1T/jsBAM7LvZCVVq9erZ07d2rv3r2aM2dObLrf79fw8LB6e3vj9qa6u7vl9/tjyxw8eDDu/s6d/XdumY/yeDzyeDwXMlQAQBqb0J6UMUarV6/Wjh07tGfPHs2dOzdufnV1tfLy8tTc3Byb1tHRoc7OTgUCAUlSIBBQe3u7enp6Ysvs3r1bXq9XVVVVk3ksAIAMM6E9qYaGBj399NN67rnnNGPGjNhnSD6fT9OnT5fP59PKlSu1bt06lZSUyOv1as2aNQoEAlqyZIkkadmyZaqqqtKdd96pjRs3KhgM6qGHHlJDQwN7SwCAOC5jjEl4YZdr3OlbtmzRF7/4RUkffpn3K1/5ip555hlFIhHV1tbq8ccfjzuU9+6772rVqlV65ZVXVFhYqPr6em3YsEG5uYk1MxwOy+fzKRQKyev1Jjp8AIAlEn0fn1CkbEGkACC9Jfo+zrX7AADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtXKdHgCA/2SM+cR5LpcrhSMB7ECkAAsYM6rR0dMKh19Ub+9ODQ0d09hYv3JzZ6uwcJFmzvwLFRRcrZwcn1wuDoAgexApwGHR6KB6e59Vd/ejOnv2oKT/3JsaGenU4ODrOn36h/L5blBp6ToVFV3HXhWyBv8kAxxkTFTvv/89dXXdr7NnD+j8QMUvN6je3h3q7LxX/f2vfOphQSCTECnAIcaM6vTprTp5cr1GR7sTWmdoqF2dnfepv//fZEx0ikcIOI9IAQ4ZGDigYPD/KhoNTWi9oaF2nTr1fzQ21js1AwMsQqQAB0SjEYVCuxSJ/PsFrd/X16yzZw9z2A8Zj0gBDhgZ+Q91d2+c1H10dt6bpNEA9iJSgAOMMTJmZJL3MZSk0QD2IlJAihlj9L+7upweBpAWiBSQYkbSLwbPOj0MIC0QKSDFOHEcSByRAlIsaozOqEQ7VTep+9miLyZnQIDFiBSQYkbSgAr0sv67euW7oPvoVIVa9KdJHRdgIyIFpNiHh/tc2q8l+rFu08gEL6H5vmZrk+7VbzRzKoYHWIVIASkWNUbGGEU0TT/UndqpuoRDFdIMfV//Sz/XH2tMOVM8UsB5XAUdSLHzT5wYUKG+rfv1gS7SZ/VTleuUxru++Yhy9SvN1VP6K+3SZ6VxlwIyD5ECUiwadykjlwZUpK36og5pka7Xy/qMXtcc/Yema0hheXVCc/Vvuk6v6r/pV/oDEShkEyIFpFhUH/+DHBFN0+uq1jHNV4HOKk8jciuqMeVoWPkaUKFGlefEcAFHESkgxT75e1IuRTRNEU1L4WgAu3HiBJBiUa5cDiSMSAEpNt7hPgDjI1JAirEnBSSOSAEpRqKAxBEpIMXGnB4AkEaIFJBixhj2poAEESkgxfhTHUDiiBSQYkQKSByRAlIsyuE+IGFECkgx9qSAxBEpIMWixkh8VwpICJECUowrTgCJI1JAihEoIHFECkixMQ71AQkjUkCKGbE3BSSKSAEpxgVmgcQRKSDFuHYfkDgiBaQYh/uAxBEpIMU43AckjkgBKcYVJ4DEESkgxfgyL5A4IgWkGIf7gMQRKSDFONwHJI5IASnGnhSQuAlFavPmzVqwYIG8Xq+8Xq8CgYB27doVmz80NKSGhgbNmjVLRUVFWr58ubq7u+Puo7OzU3V1dSooKFBpaakeeOABjY6OJufRAGmAU9CBxE0oUnPmzNGGDRvU1tam1157Tddff71uvvlmHTt2TJJ0//336/nnn9f27dvV0tKikydP6tZbb42tPzY2prq6Og0PD2vfvn168skntXXrVq1fvz65jwqwGF/mBRLnMmZyxx5KSkr0yCOP6LbbbtNFF12kp59+Wrfddpsk6a233tIVV1yh1tZWLVmyRLt27dKf//mf6+TJkyorK5MkPfHEE/rrv/5rvf/++8rPz0/od4bDYfl8PoVCIXm93skMH0i5l0Ih/cWvfqVQdHKfTlXm5+vX8+fL5XIlaWRA6iT6Pn7Bn0mNjY1p27ZtGhgYUCAQUFtbm0ZGRlRTUxNbZt68eaqsrFRra6skqbW1VVdeeWUsUJJUW1urcDgc2xsbTyQSUTgcjrsB6YpT0IHETThS7e3tKioqksfj0T333KMdO3aoqqpKwWBQ+fn5Ki4ujlu+rKxMwWBQkhQMBuMCdW7+uXmfpKmpST6fL3arqKiY6LABa3B2H5C4CUfq8ssv15EjR3TgwAGtWrVK9fX1On78+FSMLaaxsVGhUCh26+rqmtLfB0wlzu4DEpc70RXy8/N16aWXSpKqq6t16NAhPfroo7r99ts1PDys3t7euL2p7u5u+f1+SZLf79fBgwfj7u/c2X/nlhmPx+ORx+OZ6FABK3G4D0jcpL8nFY1GFYlEVF1drby8PDU3N8fmdXR0qLOzU4FAQJIUCATU3t6unp6e2DK7d++W1+tVVVXVZIcCpAX2pIDETWhPqrGxUTfeeKMqKyvV19enp59+Wq+88opefPFF+Xw+rVy5UuvWrVNJSYm8Xq/WrFmjQCCgJUuWSJKWLVumqqoq3Xnnndq4caOCwaAeeughNTQ0sKeErPGvfX0amOSZfZK0/COf/wKZaEKR6unp0Re+8AWdOnVKPp9PCxYs0Isvvqg/+7M/kyR961vfktvt1vLlyxWJRFRbW6vHH388tn5OTo527typVatWKRAIqLCwUPX19frGN76R3EcFWKxvbCwpJ0+U5OQk4V4Au036e1JO4HtSSGf1J07on86cmfT9/H15uRr9fr4nhbQ05d+TAuAs9qOQDYgUkKbc7EEhCxApIE3x4kU24HkOpKkc9qSQBYgUkKZ48SIb8DwH0hSfSSEbECkgTXF2H7IBkQLSFHtSyAZECkhTvHiRDXieA2mKPSlkAyIFpCk+k0I2IFJAmuLFi2zA8xxIUxzuQzYgUkCa4ooTyAZECkhTvHiRDXieA2mKFy+yAc9zIE1xuA/ZgEgBaYoXL7IBz3MgTXF2H7IBkQLSFF/mRTYgUkCaYk8K2YBIAWmKFy+yAc9zIE1xuA/ZgEgBaYrDfcgGRApIU0QK2YBIAWmKw33IBkQKSFO8eJENeJ4DaYrDfcgGRApIU1y7D9mASAFpihcvskGu0wMAIBWqX1fouC7VL1WskKJy631dpLf1h3pbf6gR5X9sHSKFbECkAAflakSLdEhf0A81Vyc0Q2Hla0SSNKRpCsmnds3X93S33tUlip53Th+fSSEbECnAITMU1l/o/+kubZFHEX00OQUaVIEG5VdQC9SuzVqll7QstlfFKejIBhwxABzg0ZBu1zb9lZ7StHECdT6XJL+6da8e15/qFUlGEntSyA5ECnDAErXqC/qhZqg/4XXK1KM1+o7m6oQkXrzIDjzPgRQr1IAatUEFGpzwuhfrlO7V48pXhFPQkRWIFJBiN+s5zVDfBa3rkjRfR/UZvc6LF1mB5zmQYn+ot+XR8AWvf5E+kF/dfCaFrECkgDQVNcbpIQBTjkgBaSocjTo9BGDKESkgTf1mdNTpIQBTjkgBKfamrlBknMscJapbpTqpcvWOjSVxVICdiBSQYj/R5xSS74LWNZKOar6O6CqFiBSyAJECUmxQ0/X3+qr6VDjhdbs0R99Vg0aUz+E+ZAUiBTjgNS3SP6leYc1IeJ1TKtN3tUZdqpQktQ4MTNXwAGsQKSDFarxe5boLtF3/Q1tVr7Oark87mTwql95Tub6jNdqrP5Z+e6W/Q2fPpmS8gJO4CjqQYpd5PMp3ufQbzdBTulPH9F/1V3pKl+rfVaxeeRSRkUuDmq7faKZ+oYX6gb6k/1CFDP+uRJYhUkCK+XJyYqmJKkdtukZv6gpdrrf1B/qVfL/9o4cfaLZ+qf+iX+oyjSrP0TEDTiFSQIr5cnI+dnHYsyrSYV2tw7raoVEBduLYAZBixbm5XMEcSBCRAlJsusv1qX/kcCL4phQyHZEC0lTUGA1w/T5kOCIFpKkxYxTmC73IcEQKSFNjkvrZk0KGI1JAmopK6iNSyHBECnDAvGnTJn0fY8YozEVmkeGIFOCAm3wXdhX08w1Eo9rX35+E0QD2IlKAA2bnTv579MPG6N3h4SSMBrAXkQIcUJKESAHZgEgBDijJyXF6CEBaIFKAA2ayJwUkhEgBKeZyucR+FJAYIgWksag+vDwSkKkmFakNGzbI5XJp7dq1sWlDQ0NqaGjQrFmzVFRUpOXLl6u7uztuvc7OTtXV1amgoEClpaV64IEHNMrlXYAJi0SjihApZLALjtShQ4f0D//wD1qwYEHc9Pvvv1/PP/+8tm/frpaWFp08eVK33nprbP7Y2Jjq6uo0PDysffv26cknn9TWrVu1fv36C38UQJYaMkYRrjqBDHZBkerv79eKFSv0ve99TzNnzoxND4VC+sEPfqBvfvObuv7661VdXa0tW7Zo37592r9/vyTppZde0vHjx/XUU0/pqquu0o033qi//du/1aZNmzTMdz6QJZL1udQQe1LIcBcUqYaGBtXV1ammpiZueltbm0ZGRuKmz5s3T5WVlWptbZUktba26sorr1RZWVlsmdraWoXDYR07dmzc3xeJRBQOh+NuQDqblZOjzxcXT/p+BqNRDbEnhQw24fNgt23bptdff12HDh362LxgMKj8/HwVf+TFV1ZWpmAwGFvm/ECdm39u3niampr09a9/faJDBayV63KpOAnflRoyhj0pZLQJ7Ul1dXXpvvvu049+9CNNS8IFMhPV2NioUCgUu3V1daXsdwNTIcflkjcJkXpjcFDHh4aSMCLAThOKVFtbm3p6enT11VcrNzdXubm5amlp0WOPPabc3FyVlZVpeHhYvb29cet1d3fL7/dLkvx+/8fO9jv387llPsrj8cjr9cbdgHSWKyVlT+psNKqzHO5DBptQpJYuXar29nYdOXIkdlu0aJFWrFgR+++8vDw1NzfH1uno6FBnZ6cCgYAkKRAIqL29XT09PbFldu/eLa/Xq6qqqiQ9LMBuOS6XZnBpJOB3mtBnUjNmzND8+fPjphUWFmrWrFmx6StXrtS6detUUlIir9erNWvWKBAIaMmSJZKkZcuWqaqqSnfeeac2btyoYDCohx56SA0NDfJ4PEl6WIDdXJKmufkuPfC7JP0CYt/61rfkdru1fPlyRSIR1dbW6vHHH4/Nz8nJ0c6dO7Vq1SoFAgEVFhaqvr5e3/jGN5I9FMBaLpfL6SEAacFlTPqdGhQOh+Xz+RQKhfh8Cmnrifff16rOzknfz4/mztX/LClJwoiA1En0fZzjDUCaM8YoDf+tCSSESAFprm9sTCQKmYpIAQ65zONRRV7epO+nLxoVJ6EjUxEpwCG/l5en0iREKjQ2xp/rQMYiUoBDCnJyNC0JZ/n1jY2xJ4WMRaQAh0x3ueRJwnelONyHTEakAIcUut2anoQ9qa7hYY1yuA8ZikgBDpnmdis/CXtS/9rXp0Gu34cMRaQAh7hdLnHdCeDTESkAgLWIFADAWkQKAGAtIgU4qDwvLykvwjHO7kOGIlKAg27w+ZLyd6X6xsaSMBrAPkQKcNDMnJykvAhDRAoZikgBDkpWpM4QKWQoIgU4yJeTI3cSrjrBnhQyFZECHFScm5uUF2Hv6GgS7gWwD5ECHJSMq6BL0uHBwaTcD2AbIgVkgFf6+pweAjAliBQAwFpECgBgLSIFOIwroQOfjEgBDnJJqp81y+lhANYiUoDDLs7LS8r9RLl+HzIQkQIcVpKTM+n7iEr8dV5kJCIFOGxWEvakRo1RH5FCBiJSgMOSsSc1JqmfSyMhAxEpwGHeZETKGPWzJ4UMRKQAB7lcrqRcYHbUGP6mFDISkQIyQF80yvX7kJGIFJABzkajemdoyOlhAElHpAAA1iJSgMNmuN26Yto0p4cBWIlIAQ6bkZOjeUQKGBeRAhyW63LJ6+alCIyHVwbgsFx9uDc1WUaS4fp9yDBECnBYnsuVlC/0RozRCJFChiFSgMNyXa6k7EkNRqOKEClkGCIFOMztcikvCVedGIxGNUykkGGIFJAhBqNRRbh+HzIMkQIyBHtSyERECsgQbwwO6sTwsNPDAJKKSAEWKHS75Znk51Jnxsa4EjoyDpECLFBdUKBL8vOdHgZgHSIFWKDQ7VY+V50APoZXBWCBZBzuAzIRkQIsQKSA8eU6PQAAUoHbLc9vD/e5fns7/79dvw3Y75qXS+iQYYgUYIHpbreWFBbK63arMCdHhW63itxuFbjdKjxv2vnTi347reDctJwcTSNSyDBECrCAy+XS3//e7zk9DMA6fCYFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC10vJPdRhjJEnhcNjhkQAALsS59+9z7+efJC0jdfr0aUlSRUWFwyMBAExGX1+ffD7fJ85Py0iVlJRIkjo7Oz/1wWW7cDisiooKdXV1yev1Oj0ca7GdEsN2SgzbKTHGGPX19am8vPxTl0vLSLndH36U5vP5eBIkwOv1sp0SwHZKDNspMWyn3y2RnQxOnAAAWItIAQCslZaR8ng8+trXviaPx+P0UKzGdkoM2ykxbKfEsJ2Sy2V+1/l/AAA4JC33pAAA2YFIAQCsRaQAANYiUgAAa6VlpDZt2qRLLrlE06ZN0+LFi3Xw4EGnh5RSe/fu1U033aTy8nK5XC49++yzcfONMVq/fr0uvvhiTZ8+XTU1NXrnnXfiljlz5oxWrFghr9er4uJirVy5Uv39/Sl8FFOrqalJ11xzjWbMmKHS0lLdcsst6ujoiFtmaGhIDQ0NmjVrloqKirR8+XJ1d3fHLdPZ2am6ujoVFBSotLRUDzzwgEZHR1P5UKbU5s2btWDBgtgXTwOBgHbt2hWbzzYa34YNG+RyubR27drYNLbVFDFpZtu2bSY/P9/84z/+ozl27Ji5++67TXFxsenu7nZ6aCnzwgsvmK9+9avmX/7lX4wks2PHjrj5GzZsMD6fzzz77LPmF7/4hfnc5z5n5s6dawYHB2PL3HDDDWbhwoVm//795uc//7m59NJLzR133JHiRzJ1amtrzZYtW8zRo0fNkSNHzGc/+1lTWVlp+vv7Y8vcc889pqKiwjQ3N5vXXnvNLFmyxPzRH/1RbP7o6KiZP3++qampMYcPHzYvvPCCmT17tmlsbHTiIU2Jn/zkJ+anP/2pefvtt01HR4f5m7/5G5OXl2eOHj1qjGEbjefgwYPmkksuMQsWLDD33XdfbDrbamqkXaSuvfZa09DQEPt5bGzMlJeXm6amJgdH5ZyPRioajRq/328eeeSR2LTe3l7j8XjMM888Y4wx5vjx40aSOXToUGyZXbt2GZfLZd57772UjT2Venp6jCTT0tJijPlwm+Tl5Znt27fHlnnzzTeNJNPa2mqM+fAfA2632wSDwdgymzdvNl6v10QikdQ+gBSaOXOm+f73v882GkdfX5+57LLLzO7du82f/MmfxCLFtpo6aXW4b3h4WG1tbaqpqYlNc7vdqqmpUWtrq4Mjs8eJEycUDAbjtpHP59PixYtj26i1tVXFxcVatGhRbJmamhq53W4dOHAg5WNOhVAoJOk/L07c1tamkZGRuO00b948VVZWxm2nK6+8UmVlZbFlamtrFQ6HdezYsRSOPjXGxsa0bds2DQwMKBAIsI3G0dDQoLq6urhtIvF8mkppdYHZDz74QGNjY3H/kyWprKxMb731lkOjskswGJSkcbfRuXnBYFClpaVx83Nzc1VSUhJbJpNEo1GtXbtW1113nebPny/pw22Qn5+v4uLiuGU/up3G247n5mWK9vZ2BQIBDQ0NqaioSDt27FBVVZWOHDnCNjrPtm3b9Prrr+vQoUMfm8fzaeqkVaSAC9HQ0KCjR4/q1VdfdXooVrr88st15MgRhUIh/fjHP1Z9fb1aWlqcHpZVurq6dN9992n37t2aNm2a08PJKml1uG/27NnKycn52Bkz3d3d8vv9Do3KLue2w6dtI7/fr56enrj5o6OjOnPmTMZtx9WrV2vnzp16+eWXNWfOnNh0v9+v4eFh9fb2xi3/0e003nY8Ny9T5Ofn69JLL1V1dbWampq0cOFCPfroo2yj87S1tamnp0dXX321cnNzlZubq5aWFj322GPKzc1VWVkZ22qKpFWk8vPzVV1drebm5ti0aDSq5uZmBQIBB0dmj7lz58rv98dto3A4rAMHDsS2USAQUG9vr9ra2mLL7NmzR9FoVIsXL075mKeCMUarV6/Wjh07tGfPHs2dOzdufnV1tfLy8uK2U0dHhzo7O+O2U3t7e1zQd+/eLa/Xq6qqqtQ8EAdEo1FFIhG20XmWLl2q9vZ2HTlyJHZbtGiRVqxYEftvttUUcfrMjYnatm2b8Xg8ZuvWreb48ePmy1/+sikuLo47YybT9fX1mcOHD5vDhw8bSeab3/ymOXz4sHn33XeNMR+egl5cXGyee+4588Ybb5ibb7553FPQP/OZz5gDBw6YV1991Vx22WUZdQr6qlWrjM/nM6+88oo5depU7Hb27NnYMvfcc4+prKw0e/bsMa+99poJBAImEAjE5p87ZXjZsmXmyJEj5mc/+5m56KKLMuqU4QcffNC0tLSYEydOmDfeeMM8+OCDxuVymZdeeskYwzb6NOef3WcM22qqpF2kjDHmO9/5jqmsrDT5+fnm2muvNfv373d6SCn18ssvG0kfu9XX1xtjPjwN/eGHHzZlZWXG4/GYpUuXmo6Ojrj7OH36tLnjjjtMUVGR8Xq95q677jJ9fX0OPJqpMd72kWS2bNkSW2ZwcNDce++9ZubMmaagoMB8/vOfN6dOnYq7n1//+tfmxhtvNNOnTzezZ882X/nKV8zIyEiKH83U+dKXvmR+//d/3+Tn55uLLrrILF26NBYoY9hGn+ajkWJbTQ3+VAcAwFpp9ZkUACC7ECkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGCt/w8NiZRNAVhzzwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], 16),\n",
        "    nn.GELU(),\n",
        "    nn.Linear(16, 32),\n",
        "    nn.GELU(),\n",
        "    nn.Linear(32, n_actions)\n",
        ")\n"
      ],
      "metadata": {
        "id": "717OJHiHmDlm"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], 32),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(32, 16),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(16, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "fUi21dgNxxFe"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "FHCQLGF98Vxk"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "pDNY5YrlLorI"
      },
      "outputs": [],
      "source": [
        "def get_loss(states, logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2, value_coef=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    states = torch.as_tensor(states, dtype=torch.float32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    value = value_model(states).squeeze()\n",
        "    log_probs_for_actions = log_probs.gather(1, actions.long().view(-1, 1)).squeeze(1)  # [B]\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = (log_probs_for_actions * (cumulative_returns - value.detach())).mean()  # a number\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    value_loss = F.mse_loss(value, cumulative_returns)\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = entropy_coef * (-(probs * log_probs).sum(dim=-1).mean())\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    loss = -J_hat - entropy + value_coef * value_loss\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(states, logits, actions, rewards, n_actions=n_actions)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "metadata": {
        "id": "3YaGyC435Xjy"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(15):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(200)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > -100:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "metadata": {
        "id": "GbQLNGBB5fYj",
        "outputId": "3908a89c-c63a-42ee-c710-949a5d252085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:-890.215\n",
            "mean reward:-948.345\n",
            "mean reward:-942.130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2hLn33l16GUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}